{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a74a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install vllm openai aiohttp requests numpy matplotlib pandas -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369219f0",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1faaebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import requests\n",
    "import aiohttp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cc344a",
   "metadata": {},
   "source": [
    "## Step 2: Configuration - SAME MODEL for Both Systems\n",
    "\n",
    "**Important:** Both systems are using TinyLlama for fair comparison:\n",
    "- vLLM: `TinyLlama/TinyLlama-1.1B-Chat-v1.0`\n",
    "- Ollama: `tinyllama` (same model, different naming convention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf78cb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration set:\n",
      "  vLLM Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "  Ollama Model: tinyllama\n",
      "  Both are TinyLlama 1.1B for fair comparison\n"
     ]
    }
   ],
   "source": [
    "# Configuration - SAME MODEL for fair comparison\n",
    "VLLM_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "OLLAMA_MODEL = \"tinyllama\"\n",
    "TEST_PROMPT = \"Once upon a time in a distant galaxy\"\n",
    "MAX_TOKENS = 50\n",
    "CONCURRENT_REQUESTS = [1, 2, 4, 8, 16]  # Test with different concurrency levels\n",
    "\n",
    "print(f\"‚úì Configuration set:\")\n",
    "print(f\"  vLLM Model: {VLLM_MODEL}\")\n",
    "print(f\"  Ollama Model: {OLLAMA_MODEL}\")\n",
    "print(f\"  Both are TinyLlama 1.1B for fair comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34aa939",
   "metadata": {},
   "source": [
    "## Step 3: Verify Server Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb7ed08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì vLLM server is running!\n",
      "  Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "‚úì Ollama server is running!\n",
      "  Available models:\n",
      "    - tinyllama:latest\n"
     ]
    }
   ],
   "source": [
    "# Test vLLM connection\n",
    "vllm_url = \"http://localhost:8000/v1/completions\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "    model_info = response.json()\n",
    "    print(f\"‚úì vLLM server is running!\")\n",
    "    print(f\"  Model: {model_info['data'][0]['id']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó vLLM server not detected: {e}\")\n",
    "\n",
    "# Test Ollama connection\n",
    "ollama_url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:11434\")\n",
    "    print(\"‚úì Ollama server is running!\")\n",
    "    \n",
    "    # List available models\n",
    "    models_response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "    models = models_response.json()\n",
    "    print(\"  Available models:\")\n",
    "    for model in models.get('models', []):\n",
    "        print(f\"    - {model['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Ollama server not detected: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e556f4",
   "metadata": {},
   "source": [
    "## Step 4: Define Benchmark Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb080e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Benchmark functions defined\n"
     ]
    }
   ],
   "source": [
    "def test_vllm_single(prompt, max_tokens=50):\n",
    "    \"\"\"Single synchronous request to vLLM\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": VLLM_MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    response = requests.post(vllm_url, json=payload)\n",
    "    result = response.json()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # Extract token count from response\n",
    "    tokens = result['usage']['completion_tokens']\n",
    "    \n",
    "    return {\n",
    "        'tokens': tokens,\n",
    "        'duration': duration,\n",
    "        'tokens_per_sec': tokens / duration,\n",
    "        'response': result['choices'][0]['text']\n",
    "    }\n",
    "\n",
    "def test_ollama_single(prompt, max_tokens=50):\n",
    "    \"\"\"Single synchronous request to Ollama\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": OLLAMA_MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"num_predict\": max_tokens,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(ollama_url, json=payload)\n",
    "    result = response.json()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # Ollama returns eval_count for tokens generated\n",
    "    tokens = result.get('eval_count', max_tokens)\n",
    "    \n",
    "    return {\n",
    "        'tokens': tokens,\n",
    "        'duration': duration,\n",
    "        'tokens_per_sec': tokens / duration,\n",
    "        'response': result.get('response', '')\n",
    "    }\n",
    "\n",
    "print(\"‚úì Benchmark functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e54ce5",
   "metadata": {},
   "source": [
    "## Step 5: Warm-up Requests\n",
    "\n",
    "Run a few warm-up requests to ensure both servers are ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "425a0c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up servers...\n",
      "\n",
      "1. Testing vLLM:\n",
      "   ‚úì Response time: 0.45s\n",
      "   ‚úì Tokens/sec: 22.34\n",
      "\n",
      "2. Testing Ollama:\n",
      "   ‚úì Response time: 1.35s\n",
      "   ‚úì Tokens/sec: 7.39\n",
      "\n",
      "‚úì Warm-up complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Warming up servers...\")\n",
    "print(\"\\n1. Testing vLLM:\")\n",
    "try:\n",
    "    result = test_vllm_single(\"Hello\", 10)\n",
    "    print(f\"   ‚úì Response time: {result['duration']:.2f}s\")\n",
    "    print(f\"   ‚úì Tokens/sec: {result['tokens_per_sec']:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úó Error: {e}\")\n",
    "\n",
    "print(\"\\n2. Testing Ollama:\")\n",
    "try:\n",
    "    result = test_ollama_single(\"Hello\", 10)\n",
    "    print(f\"   ‚úì Response time: {result['duration']:.2f}s\")\n",
    "    print(f\"   ‚úì Tokens/sec: {result['tokens_per_sec']:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úó Error: {e}\")\n",
    "\n",
    "print(\"\\n‚úì Warm-up complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47726eaf",
   "metadata": {},
   "source": [
    "## Step 6: Concurrent Benchmark Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eb250e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Concurrent benchmark function defined\n"
     ]
    }
   ],
   "source": [
    "def benchmark_concurrent(test_func, num_concurrent, num_requests=10):\n",
    "    \"\"\"Run concurrent requests and measure throughput\"\"\"\n",
    "    print(f\"  Testing with {num_concurrent} concurrent requests...\")\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_concurrent) as executor:\n",
    "        futures = []\n",
    "        for i in range(num_requests):\n",
    "            future = executor.submit(test_func, TEST_PROMPT, MAX_TOKENS)\n",
    "            futures.append(future)\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"    Request failed: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_duration = end_time - start_time\n",
    "    \n",
    "    if not results:\n",
    "        return None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_tokens = sum(r['tokens'] for r in results)\n",
    "    avg_tokens_per_sec = np.mean([r['tokens_per_sec'] for r in results])\n",
    "    throughput = total_tokens / total_duration\n",
    "    \n",
    "    return {\n",
    "        'num_concurrent': num_concurrent,\n",
    "        'num_requests': len(results),\n",
    "        'total_duration': total_duration,\n",
    "        'total_tokens': total_tokens,\n",
    "        'avg_tokens_per_sec': avg_tokens_per_sec,\n",
    "        'throughput': throughput,\n",
    "        'avg_latency': np.mean([r['duration'] for r in results]),\n",
    "        'min_latency': np.min([r['duration'] for r in results]),\n",
    "        'max_latency': np.max([r['duration'] for r in results])\n",
    "    }\n",
    "\n",
    "print(\"‚úì Concurrent benchmark function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c38263c",
   "metadata": {},
   "source": [
    "## Step 7: Run vLLM Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "024f54c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BENCHMARKING vLLM (TinyLlama)\n",
      "======================================================================\n",
      "  Testing with 1 concurrent requests...\n",
      "  Concurrency 1:\n",
      "    Throughput: 62.91 tokens/sec\n",
      "    Avg latency: 0.79s\n",
      "    Min/Max latency: 0.47s / 1.60s\n",
      "  Testing with 2 concurrent requests...\n",
      "  Concurrency 2:\n",
      "    Throughput: 351.29 tokens/sec\n",
      "    Avg latency: 0.28s\n",
      "    Min/Max latency: 0.25s / 0.36s\n",
      "  Testing with 4 concurrent requests...\n",
      "  Concurrency 4:\n",
      "    Throughput: 713.08 tokens/sec\n",
      "    Avg latency: 0.28s\n",
      "    Min/Max latency: 0.23s / 0.35s\n",
      "  Testing with 8 concurrent requests...\n",
      "  Concurrency 8:\n",
      "    Throughput: 1249.36 tokens/sec\n",
      "    Avg latency: 0.26s\n",
      "    Min/Max latency: 0.25s / 0.29s\n",
      "  Testing with 16 concurrent requests...\n",
      "  Concurrency 16:\n",
      "    Throughput: 1653.34 tokens/sec\n",
      "    Avg latency: 0.32s\n",
      "    Min/Max latency: 0.26s / 0.35s\n",
      "\n",
      "‚úì vLLM benchmarks complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"BENCHMARKING vLLM (TinyLlama)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "vllm_results = []\n",
    "\n",
    "for num_concurrent in CONCURRENT_REQUESTS:\n",
    "    result = benchmark_concurrent(test_vllm_single, num_concurrent, num_requests=20)\n",
    "    if result:\n",
    "        vllm_results.append(result)\n",
    "        print(f\"  Concurrency {num_concurrent}:\")\n",
    "        print(f\"    Throughput: {result['throughput']:.2f} tokens/sec\")\n",
    "        print(f\"    Avg latency: {result['avg_latency']:.2f}s\")\n",
    "        print(f\"    Min/Max latency: {result['min_latency']:.2f}s / {result['max_latency']:.2f}s\")\n",
    "\n",
    "print(\"\\n‚úì vLLM benchmarks complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb51376d",
   "metadata": {},
   "source": [
    "## Step 8: Run Ollama Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a114e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BENCHMARKING OLLAMA (TinyLlama)\n",
      "======================================================================\n",
      "  Testing with 1 concurrent requests...\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"BENCHMARKING OLLAMA (TinyLlama)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ollama_results = []\n",
    "\n",
    "for num_concurrent in CONCURRENT_REQUESTS:\n",
    "    result = benchmark_concurrent(test_ollama_single, num_concurrent, num_requests=20)\n",
    "    if result:\n",
    "        ollama_results.append(result)\n",
    "        print(f\"  Concurrency {num_concurrent}:\")\n",
    "        print(f\"    Throughput: {result['throughput']:.2f} tokens/sec\")\n",
    "        print(f\"    Avg latency: {result['avg_latency']:.2f}s\")\n",
    "        print(f\"    Min/Max latency: {result['min_latency']:.2f}s / {result['max_latency']:.2f}s\")\n",
    "\n",
    "print(\"\\n‚úì Ollama benchmarks complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47850fb",
   "metadata": {},
   "source": [
    "## Step 9: Compare Results (Same Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700b9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for vllm_r, ollama_r in zip(vllm_results, ollama_results):\n",
    "    comparison_data.append({\n",
    "        'Concurrent Requests': vllm_r['num_concurrent'],\n",
    "        'vLLM Throughput (tok/s)': vllm_r['throughput'],\n",
    "        'Ollama Throughput (tok/s)': ollama_r['throughput'],\n",
    "        'vLLM Latency (s)': vllm_r['avg_latency'],\n",
    "        'Ollama Latency (s)': ollama_r['avg_latency'],\n",
    "        'Throughput Ratio (vLLM/Ollama)': vllm_r['throughput'] / ollama_r['throughput']\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"FAIR COMPARISON RESULTS (Both using TinyLlama 1.1B)\")\n",
    "print(\"=\"*90)\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3d1e8",
   "metadata": {},
   "source": [
    "## Step 10: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb1a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Throughput comparison\n",
    "x = df['Concurrent Requests']\n",
    "ax1.plot(x, df['vLLM Throughput (tok/s)'], marker='o', label='vLLM', linewidth=2, markersize=8)\n",
    "ax1.plot(x, df['Ollama Throughput (tok/s)'], marker='s', label='Ollama', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Concurrent Requests', fontsize=11)\n",
    "ax1.set_ylabel('Throughput (tokens/sec)', fontsize=11)\n",
    "ax1.set_title('Throughput Comparison (Higher is Better)', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Latency comparison\n",
    "ax2.plot(x, df['vLLM Latency (s)'], marker='o', label='vLLM', linewidth=2, markersize=8)\n",
    "ax2.plot(x, df['Ollama Latency (s)'], marker='s', label='Ollama', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Concurrent Requests', fontsize=11)\n",
    "ax2.set_ylabel('Average Latency (seconds)', fontsize=11)\n",
    "ax2.set_title('Latency Comparison (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Speedup ratio\n",
    "ax3.bar(x, df['Throughput Ratio (vLLM/Ollama)'], color='steelblue', alpha=0.7)\n",
    "ax3.axhline(y=1.0, color='r', linestyle='--', label='Equal Performance')\n",
    "ax3.set_xlabel('Concurrent Requests', fontsize=11)\n",
    "ax3.set_ylabel('Throughput Ratio (vLLM/Ollama)', fontsize=11)\n",
    "ax3.set_title('Relative Performance (>1 means vLLM is faster)', fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Summary statistics table\n",
    "summary_text = f\"\"\"\n",
    "SUMMARY (Same Model: TinyLlama 1.1B)\n",
    "{'='*40}\n",
    "\n",
    "Average Throughput:\n",
    "  vLLM:   {df['vLLM Throughput (tok/s)'].mean():.2f} tok/s\n",
    "  Ollama: {df['Ollama Throughput (tok/s)'].mean():.2f} tok/s\n",
    "\n",
    "Average Latency:\n",
    "  vLLM:   {df['vLLM Latency (s)'].mean():.2f}s\n",
    "  Ollama: {df['Ollama Latency (s)'].mean():.2f}s\n",
    "\n",
    "Avg Throughput Ratio: {df['Throughput Ratio (vLLM/Ollama)'].mean():.2f}x\n",
    "\n",
    "Winner: {'vLLM' if df['vLLM Throughput (tok/s)'].mean() > df['Ollama Throughput (tok/s)'].mean() else 'Ollama'}\n",
    "\"\"\"\n",
    "\n",
    "ax4.axis('off')\n",
    "ax4.text(0.1, 0.5, summary_text, fontsize=11, family='monospace', \n",
    "         verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "plt.suptitle('vLLM vs Ollama: Fair Comparison with Same Model', fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('vllm_vs_ollama_fair_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization saved as 'vllm_vs_ollama_fair_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e39f13",
   "metadata": {},
   "source": [
    "## Step 11: Detailed Summary\n",
    "\n",
    "### Key Findings (Apple-to-Apple Comparison):\n",
    "Both systems tested with **TinyLlama 1.1B** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd550286",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"DETAILED SUMMARY - FAIR COMPARISON\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "avg_vllm_throughput = df['vLLM Throughput (tok/s)'].mean()\n",
    "avg_ollama_throughput = df['Ollama Throughput (tok/s)'].mean()\n",
    "avg_ratio = df['Throughput Ratio (vLLM/Ollama)'].mean()\n",
    "\n",
    "print(f\"\\nModel Used:\")\n",
    "print(f\"  vLLM:   {VLLM_MODEL}\")\n",
    "print(f\"  Ollama: {OLLAMA_MODEL} (same model, different naming)\")\n",
    "\n",
    "print(f\"\\nAverage Throughput:\")\n",
    "print(f\"  vLLM:   {avg_vllm_throughput:.2f} tokens/sec\")\n",
    "print(f\"  Ollama: {avg_ollama_throughput:.2f} tokens/sec\")\n",
    "print(f\"  Ratio:  {avg_ratio:.2f}x\")\n",
    "\n",
    "print(f\"\\nAverage Latency:\")\n",
    "print(f\"  vLLM:   {df['vLLM Latency (s)'].mean():.2f}s\")\n",
    "print(f\"  Ollama: {df['Ollama Latency (s)'].mean():.2f}s\")\n",
    "\n",
    "winner = \"vLLM\" if avg_vllm_throughput > avg_ollama_throughput else \"Ollama\"\n",
    "advantage = abs(avg_vllm_throughput - avg_ollama_throughput)\n",
    "advantage_pct = (advantage / min(avg_vllm_throughput, avg_ollama_throughput)) * 100\n",
    "\n",
    "print(f\"\\nüèÜ Winner: {winner}\")\n",
    "print(f\"   Advantage: {advantage:.2f} tokens/sec ({advantage_pct:.1f}% faster)\")\n",
    "\n",
    "print(f\"\\nTest Configuration:\")\n",
    "print(f\"  Max Tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Prompt: '{TEST_PROMPT}'\")\n",
    "print(f\"  Concurrent Levels: {CONCURRENT_REQUESTS}\")\n",
    "print(f\"  Requests per level: 20\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa21c6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This benchmark provides a **fair apple-to-apple comparison** by using the same model (TinyLlama 1.1B) for both vLLM and Ollama.\n",
    "\n",
    "### Why This Comparison Matters:\n",
    "- **Same Model**: Eliminates model architecture differences\n",
    "- **Same Parameters**: Both using 1.1B parameter model\n",
    "- **Same Hardware**: Running on the same GPU\n",
    "- **Same Prompts**: Identical test conditions\n",
    "\n",
    "### Typical Results:\n",
    "- **vLLM** usually shows better throughput at higher concurrency due to PagedAttention and continuous batching\n",
    "- **Ollama** may have lower latency for single requests and is easier to set up\n",
    "- Performance differences depend on hardware, model size, and concurrency level\n",
    "\n",
    "### Important Notes:\n",
    "1. Results may vary based on GPU, VRAM, and system configuration\n",
    "2. vLLM is optimized for high-throughput serving\n",
    "3. Ollama is optimized for ease of use and local deployment\n",
    "4. Both are excellent tools for different use cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
